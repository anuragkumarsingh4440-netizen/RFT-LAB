{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "G0JyzSA_p3gd"
   },
   "source": [
    "# RFT-Lab — Understanding Encoder\n",
    "\n",
    "Before an AI system can reason or generate answers,\n",
    "it must first **understand** the input.\n",
    "\n",
    "This notebook implements a **Transformer Encoder**\n",
    "responsible only for understanding:\n",
    "\n",
    "• Context building  \n",
    "• Token relationships  \n",
    "• Semantic representation  \n",
    "\n",
    "❌ No reasoning  \n",
    "❌ No decoding  \n",
    "❌ No text generation  \n",
    "\n",
    "This encoder closely follows the\n",
    "**standard Transformer encoder architecture**,\n",
    "implemented in a simple and transparent way.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HpsDx68IqV-4"
   },
   "source": [
    "## Step 0: Imports\n",
    "\n",
    "We use only core PyTorch modules.\n",
    "This keeps the code readable and interview-friendly.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "executionInfo": {
     "elapsed": 3,
     "status": "ok",
     "timestamp": 1767110138076,
     "user": {
      "displayName": "Anurag Singh",
      "userId": "12042435789066891421"
     },
     "user_tz": -330
    },
    "id": "yEUw_7rXqXhj"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import math"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Q07shJifqcr2"
   },
   "source": [
    "# RFT-Lab — Understanding Encoder (Production Version)\n",
    "\n",
    "This notebook implements the **final Understanding Encoder**\n",
    "used in the real-time RFT system.\n",
    "\n",
    "STRICT CONSTRAINTS:\n",
    "- No assumptions\n",
    "- No placeholder inputs\n",
    "- No future replacement logic\n",
    "\n",
    "This encoder will receive **raw cleaned text**\n",
    "from the input-handling layer and convert it into\n",
    "stable, contextual representations.\n",
    "\n",
    "This module is **locked after implementation**.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nrrFVEkfrfLJ"
   },
   "source": [
    "## Step 0: Imports\n",
    "\n",
    "We use stable, widely-adopted libraries only.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "executionInfo": {
     "elapsed": 38,
     "status": "ok",
     "timestamp": 1767110138132,
     "user": {
      "displayName": "Anurag Singh",
      "userId": "12042435789066891421"
     },
     "user_tz": -330
    },
    "id": "5n8mErYGqXev"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import math\n",
    "\n",
    "from transformers import GPT2TokenizerFast"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eMoTco6crlje"
   },
   "source": [
    "## Step 1: Tokenization (Production-Fixed)\n",
    "\n",
    "Tokenization is NOT a toy decision.\n",
    "\n",
    "In real-time systems:\n",
    "- Token IDs must be consistent\n",
    "- Vocabulary must be fixed\n",
    "- Encoder must see the same distribution always\n",
    "\n",
    "We use a **pretrained GPT-2 tokenizer**\n",
    "as a stable, industry-proven choice.\n",
    "\n",
    "This tokenizer is now a **locked dependency**.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "executionInfo": {
     "elapsed": 376,
     "status": "ok",
     "timestamp": 1767110138521,
     "user": {
      "displayName": "Anurag Singh",
      "userId": "12042435789066891421"
     },
     "user_tz": -330
    },
    "id": "sNBCvuz5qXbk"
   },
   "outputs": [],
   "source": [
    "tokenizer = GPT2TokenizerFast.from_pretrained(\"gpt2\")\n",
    "tokenizer.pad_token = tokenizer.eos_token"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "On69c8c-r0Nl"
   },
   "source": [
    "## Tokenization Test (Real Input)\n",
    "\n",
    "This is exactly how user text will enter the encoder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 6,
     "status": "ok",
     "timestamp": 1767110184117,
     "user": {
      "displayName": "Anurag Singh",
      "userId": "12042435789066891421"
     },
     "user_tz": -330
    },
    "id": "BV6ZW5wpqXYr",
    "outputId": "44d063d1-d49f-47eb-cbab-0cbb47c25c02"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[37702,  2736,   428, 15294,   290,  7238, 20256,    13]]),\n",
       " tensor([[1, 1, 1, 1, 1, 1, 1, 1]]))"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Sample input text from user\n",
    "user_text = \"Analyze this resume and highlight weaknesses.\"\n",
    "\n",
    "# Tokenize text into model-ready tensors\n",
    "tokens = tokenizer(\n",
    "    user_text,\n",
    "    return_tensors=\"pt\",\n",
    "    padding=True,\n",
    "    truncation=True\n",
    ")\n",
    "\n",
    "# Extract token IDs\n",
    "input_ids = tokens[\"input_ids\"]\n",
    "\n",
    "# Extract attention mask (1 = real token, 0 = padding)\n",
    "attention_mask = tokens[\"attention_mask\"]\n",
    "\n",
    "# Return inputs for encoder/model\n",
    "input_ids, attention_mask\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "b6p0exJUsIRT"
   },
   "source": [
    "## Step 2: Token Embedding\n",
    "\n",
    "Embeddings map real token IDs into vector space.\n",
    "\n",
    "This layer will NEVER change once deployed.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "executionInfo": {
     "elapsed": 9,
     "status": "ok",
     "timestamp": 1767110205239,
     "user": {
      "displayName": "Anurag Singh",
      "userId": "12042435789066891421"
     },
     "user_tz": -330
    },
    "id": "zcCO7ionqXEH"
   },
   "outputs": [],
   "source": [
    "class TokenEmbedding(nn.Module):\n",
    "    def __init__(self, vocab_size, d_model):\n",
    "        super().__init__()\n",
    "\n",
    "        # Lookup table mapping token IDs to dense vectors\n",
    "        self.embedding = nn.Embedding(vocab_size, d_model)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Convert token IDs into embedding vectors\n",
    "        return self.embedding(x)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2fmUv2PRsNfi"
   },
   "source": [
    "## Step 3: Positional Encoding\n",
    "\n",
    "* Order information is injected deterministically.\n",
    "* This ensures repeatable behavior across runs.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "executionInfo": {
     "elapsed": 45,
     "status": "ok",
     "timestamp": 1767110328057,
     "user": {
      "displayName": "Anurag Singh",
      "userId": "12042435789066891421"
     },
     "user_tz": -330
    },
    "id": "CCR3LHoeqXBb"
   },
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model, max_len=1024):\n",
    "        super().__init__()\n",
    "\n",
    "        # Create positional encoding matrix\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "\n",
    "        # Position indices (0, 1, 2, ...)\n",
    "        position = torch.arange(0, max_len).unsqueeze(1)\n",
    "\n",
    "        # Frequency scaling for sine and cosine\n",
    "        div_term = torch.exp(\n",
    "            torch.arange(0, d_model, 2) * (-math.log(10000.0) / d_model)\n",
    "        )\n",
    "\n",
    "        # Apply sine to even indices\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "\n",
    "        # Apply cosine to odd indices\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "\n",
    "        # Store as non-trainable buffer\n",
    "        self.register_buffer(\"pe\", pe)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Add positional encoding to token embeddings\n",
    "        return x + self.pe[:x.size(1)]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "raCNUgHJsoX1"
   },
   "source": [
    "## Step 4: Self-Attention\n",
    "\n",
    "Self-attention builds contextual understanding.\n",
    "No generation. No reasoning.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "executionInfo": {
     "elapsed": 28,
     "status": "ok",
     "timestamp": 1767110399818,
     "user": {
      "displayName": "Anurag Singh",
      "userId": "12042435789066891421"
     },
     "user_tz": -330
    },
    "id": "xFMtUppg6xRx"
   },
   "outputs": [],
   "source": [
    "class SelfAttention(nn.Module):\n",
    "    def __init__(self, d_model):\n",
    "        super().__init__()\n",
    "\n",
    "        # Linear projections for queries, keys, and values\n",
    "        self.W_q = nn.Linear(d_model, d_model)\n",
    "        self.W_k = nn.Linear(d_model, d_model)\n",
    "        self.W_v = nn.Linear(d_model, d_model)\n",
    "\n",
    "    def forward(self, x, mask=None):\n",
    "        # Project input embeddings into Q, K, V\n",
    "        Q = self.W_q(x)\n",
    "        K = self.W_k(x)\n",
    "        V = self.W_v(x)\n",
    "\n",
    "        # Scaled dot-product attention scores\n",
    "        scores = torch.matmul(Q, K.transpose(-2, -1)) / math.sqrt(x.size(-1))\n",
    "\n",
    "        # Apply attention mask to ignore padding tokens\n",
    "        if mask is not None:\n",
    "            scores = scores.masked_fill(mask == 0, -1e9)\n",
    "\n",
    "        # Convert scores to attention weights\n",
    "        weights = torch.softmax(scores, dim=-1)\n",
    "\n",
    "        # Weighted sum of values\n",
    "        return torch.matmul(weights, V)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "da3C1J_As8Y2"
   },
   "source": [
    "## Step 5: Encoder Block\n",
    "\n",
    "This block is frozen after validation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "executionInfo": {
     "elapsed": 46,
     "status": "ok",
     "timestamp": 1767110500315,
     "user": {
      "displayName": "Anurag Singh",
      "userId": "12042435789066891421"
     },
     "user_tz": -330
    },
    "id": "oeE76Kts602o"
   },
   "outputs": [],
   "source": [
    "class EncoderBlock(nn.Module):\n",
    "    def __init__(self, d_model):\n",
    "        super().__init__()\n",
    "\n",
    "        # Self-attention layer for contextual interaction\n",
    "        self.attn = SelfAttention(d_model)\n",
    "\n",
    "        # Layer normalization after attention\n",
    "        self.norm1 = nn.LayerNorm(d_model)\n",
    "\n",
    "        # Feed-forward network for non-linear transformation\n",
    "        self.ffn = nn.Sequential(\n",
    "            nn.Linear(d_model, d_model * 4),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(d_model * 4, d_model)\n",
    "        )\n",
    "\n",
    "        # Layer normalization after feed-forward\n",
    "        self.norm2 = nn.LayerNorm(d_model)\n",
    "\n",
    "    def forward(self, x, mask):\n",
    "        # Residual connection + normalization after attention\n",
    "        x = self.norm1(x + self.attn(x, mask))\n",
    "\n",
    "        # Residual connection + normalization after FFN\n",
    "        x = self.norm2(x + self.ffn(x))\n",
    "\n",
    "        # Output of one encoder block\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nQqFYYL_uMZk"
   },
   "source": [
    "## Step 6: Understanding Encoder/Contexual Representation\n",
    "\n",
    "This is the final understanding module\n",
    "used by the real-time RFT system.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "executionInfo": {
     "elapsed": 12,
     "status": "ok",
     "timestamp": 1767110138596,
     "user": {
      "displayName": "Anurag Singh",
      "userId": "12042435789066891421"
     },
     "user_tz": -330
    },
    "id": "6nsTbek_8BtI"
   },
   "outputs": [],
   "source": [
    "class UnderstandingEncoder(nn.Module):\n",
    "    def __init__(self, vocab_size, d_model, num_layers):\n",
    "        super().__init__()\n",
    "\n",
    "        # Token IDs → dense embeddings\n",
    "        self.embedding = TokenEmbedding(vocab_size, d_model)\n",
    "\n",
    "        # Adds sequence position information\n",
    "        self.position = PositionalEncoding(d_model)\n",
    "\n",
    "        # Stack of encoder blocks\n",
    "        self.layers = nn.ModuleList(\n",
    "            [EncoderBlock(d_model) for _ in range(num_layers)]\n",
    "        )\n",
    "\n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        # Embed tokens\n",
    "        x = self.embedding(input_ids)\n",
    "\n",
    "        # Add positional encoding\n",
    "        x = self.position(x)\n",
    "\n",
    "        # Apply encoder layers with attention mask\n",
    "        for layer in self.layers:\n",
    "            x = layer(x, attention_mask.unsqueeze(1))\n",
    "\n",
    "        # Output contextual representations\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lcz5fUVAuO4j"
   },
   "source": [
    "## Step 7: End-to-End Understanding Test (Production)\n",
    "\n",
    "This is the exact path used in deployment.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 194,
     "status": "ok",
     "timestamp": 1767110138791,
     "user": {
      "displayName": "Anurag Singh",
      "userId": "12042435789066891421"
     },
     "user_tz": -330
    },
    "id": "SXjwq3IZwEv0",
    "outputId": "83cfa060-5aad-4280-9bb3-f52844e46f1a"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 8, 128])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Initialize encoder with vocabulary size, embedding dimension, and number of layers\n",
    "encoder = UnderstandingEncoder(\n",
    "    vocab_size=tokenizer.vocab_size,\n",
    "    d_model=128,\n",
    "    num_layers=2\n",
    ")\n",
    "\n",
    "# Pass token IDs and attention mask through the encoder\n",
    "encoded_output = encoder(input_ids, attention_mask)\n",
    "\n",
    "# Shape: (batch_size, sequence_length, d_model)\n",
    "encoded_output.shape\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wF1Aa3dBuSEE"
   },
   "source": [
    "# **This notebook shows:**\n",
    "- No assumptions\n",
    "- Fixed tokenizer contract\n",
    "- Real-time safe design\n",
    "- Production discipline\n",
    "- No future rewrites"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "executionInfo": {
     "elapsed": 12,
     "status": "ok",
     "timestamp": 1767110138792,
     "user": {
      "displayName": "Anurag Singh",
      "userId": "12042435789066891421"
     },
     "user_tz": -330
    },
    "id": "JL2FL0EJqW7r"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "executionInfo": {
     "elapsed": 12,
     "status": "ok",
     "timestamp": 1767110138794,
     "user": {
      "displayName": "Anurag Singh",
      "userId": "12042435789066891421"
     },
     "user_tz": -330
    },
    "id": "Wr9aIH7gpxLO"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyMenQhI99MVaerWUU/e+YdF",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
