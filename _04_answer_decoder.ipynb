{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "z8W9ym6Hdajr"
   },
   "source": [
    "# RFT-Lab — Answer Decoder\n",
    "\n",
    "This notebook implements the **Answer Decoder**.\n",
    "\n",
    "Important rule:\n",
    "- Generation happens ONLY after reasoning is complete\n",
    "\n",
    "Pipeline so far:\n",
    "Input → Understanding Encoder → Reasoning Block → Answer Decoder\n",
    "\n",
    "This decoder:\n",
    "- Converts reasoned latent state into text\n",
    "- Does NOT reason\n",
    "- Does NOT modify reasoning output\n",
    "- Produces short, controlled answers\n",
    "\n",
    "This module is frozen after validation.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TH8QK798dnPl"
   },
   "source": [
    "## Step 0: Imports\n",
    "\n",
    "We use PyTorch for decoding\n",
    "and a pretrained tokenizer for text conversion.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "executionInfo": {
     "elapsed": 12541,
     "status": "ok",
     "timestamp": 1767118392221,
     "user": {
      "displayName": "Anurag Singh",
      "userId": "12042435789066891421"
     },
     "user_tz": -330
    },
    "id": "YqddpQ-edaXA"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from transformers import GPT2TokenizerFast"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mx4CPmZGdrfj"
   },
   "source": [
    "## Step 1: Tokenizer\n",
    "\n",
    "The decoder must convert latent outputs into text.\n",
    "\n",
    "We use a fixed GPT-2 tokenizer.\n",
    "This ensures consistency between training and inference.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "executionInfo": {
     "elapsed": 991,
     "status": "ok",
     "timestamp": 1767118913159,
     "user": {
      "displayName": "Anurag Singh",
      "userId": "12042435789066891421"
     },
     "user_tz": -330
    },
    "id": "y5B2SeWudwaa"
   },
   "outputs": [],
   "source": [
    "# Load GPT-2 tokenizer for fast text tokenization\n",
    "tokenizer = GPT2TokenizerFast.from_pretrained(\"gpt2\")\n",
    "\n",
    "# Set padding token to end-of-sequence token (GPT-2 has no native pad token)\n",
    "tokenizer.pad_token = tokenizer.eos_token # blank space is padding ,we use EOS token as that blank space"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yk8QH9H8gCnJ"
   },
   "source": [
    "## Step 2: Decoder Head\n",
    "\n",
    "This layer maps latent vectors\n",
    "to vocabulary logits.\n",
    "\n",
    "Important:\n",
    "- This layer does NOT reason\n",
    "- It only translates representations into words\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "executionInfo": {
     "elapsed": 4,
     "status": "ok",
     "timestamp": 1767119150216,
     "user": {
      "displayName": "Anurag Singh",
      "userId": "12042435789066891421"
     },
     "user_tz": -330
    },
    "id": "cE1uHHmYgFvV"
   },
   "outputs": [],
   "source": [
    "class AnswerDecoderHead(nn.Module):\n",
    "    def __init__(self, d_model, vocab_size):\n",
    "        super().__init__()\n",
    "\n",
    "        # Converts internal representations into vocabulary scores\n",
    "        self.output_layer = nn.Linear(d_model, vocab_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Input shape: (batch, seq_len, d_model)\n",
    "        # Output shape: (batch, seq_len, vocab_size)\n",
    "        logits = self.output_layer(x)\n",
    "        return logits\n",
    "# This decoder head is the mouth of the model.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZFk8VntBgoTV"
   },
   "source": [
    "## Step 3: Controlled Decoding Strategy\n",
    "\n",
    "We want:\n",
    "- Short answers\n",
    "- Stable outputs\n",
    "- No hallucinated long text\n",
    "\n",
    "So we use:\n",
    "- Argmax decoding (deterministic)\n",
    "- No temperature tricks\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "executionInfo": {
     "elapsed": 4,
     "status": "ok",
     "timestamp": 1767119186056,
     "user": {
      "displayName": "Anurag Singh",
      "userId": "12042435789066891421"
     },
     "user_tz": -330
    },
    "id": "9C9GU8X2grnC"
   },
   "outputs": [],
   "source": [
    "def decode_tokens(logits):\n",
    "    # Pick the most probable token at each position\n",
    "    token_ids = torch.argmax(logits, dim=-1)\n",
    "    return token_ids\n",
    "# We keep decoding simple and deterministic for safety"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gKL1Ilo5gwW0"
   },
   "source": [
    "## Step 4: Answer Decoder\n",
    "\n",
    "This module:\n",
    "- Accepts reasoned latent state\n",
    "- Generates final text output\n",
    "- Does not modify reasoning\n",
    "\n",
    "This is the LAST step in the pipeline.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "executionInfo": {
     "elapsed": 3,
     "status": "ok",
     "timestamp": 1767119299810,
     "user": {
      "displayName": "Anurag Singh",
      "userId": "12042435789066891421"
     },
     "user_tz": -330
    },
    "id": "yYhfEebfgzaP"
   },
   "outputs": [],
   "source": [
    "class AnswerDecoder(nn.Module):\n",
    "    def __init__(self, d_model, vocab_size):\n",
    "        super().__init__()\n",
    "\n",
    "        # Decoder head converts latent state to logits\n",
    "        self.decoder_head = AnswerDecoderHead(d_model, vocab_size)\n",
    "\n",
    "    def forward(self, reasoned_state):\n",
    "        # Convert latent representation to logits\n",
    "        logits = self.decoder_head(reasoned_state)\n",
    "\n",
    "        # Convert logits to token IDs\n",
    "        token_ids = decode_tokens(logits)\n",
    "\n",
    "        return token_ids\n",
    "    # This AnswerDecoder is the speaking part of the model.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LClNuvBPhML5"
   },
   "source": [
    "## Step 5: End-to-End Test\n",
    "\n",
    "We simulate the output of the Reasoning Block\n",
    "and decode it into text.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 125,
     "status": "ok",
     "timestamp": 1767119324731,
     "user": {
      "displayName": "Anurag Singh",
      "userId": "12042435789066891421"
     },
     "user_tz": -330
    },
    "id": "Hixp_LlIhPzu",
    "outputId": "c7785d8b-68ec-4314-be59-fce9060d840c"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[' Prometheuswives Cust Feet Tree ']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Simulated reasoned output from Reasoning Block\n",
    "reasoned_state = torch.randn(1, 6, 128)\n",
    "\n",
    "# Initialize decoder\n",
    "decoder = AnswerDecoder(\n",
    "    d_model=128,\n",
    "    vocab_size=tokenizer.vocab_size\n",
    ")\n",
    "\n",
    "# Decode tokens\n",
    "token_ids = decoder(reasoned_state)\n",
    "\n",
    "# Convert tokens to text\n",
    "decoded_text = tokenizer.batch_decode(\n",
    "    token_ids,\n",
    "    skip_special_tokens=True\n",
    ")\n",
    "\n",
    "decoded_text\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ta81zS1ehaLt"
   },
   "source": [
    "## Output Explanation\n",
    "\n",
    "The decoder returns:\n",
    "- token_ids → predicted vocabulary tokens\n",
    "- decoded_text → final user-visible answer\n",
    "\n",
    "Key property:\n",
    "Reasoning has already happened.\n",
    "The decoder only verbalizes it.\n",
    "\n",
    "This guarantees:\n",
    "- No hidden reasoning during generation\n",
    "- Clear separation of responsibilities\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "K17i2NQRhhup"
   },
   "source": [
    "# **This notebook demonstrates:**\n",
    "- Strict separation of reasoning and generation\n",
    "- Safe, deterministic decoding\n",
    "- Production-ready design\n",
    "- Explainable AI pipeline"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyOwgkBArNo0um0YvcB1lRXm",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
